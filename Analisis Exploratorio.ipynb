{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos históricos:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   ID                               10000 non-null  int64  \n",
      " 1   CreditScore                      10000 non-null  int64  \n",
      " 2   DebtRatio                        10000 non-null  float64\n",
      " 3   Assets                           10000 non-null  int64  \n",
      " 4   Age                              10000 non-null  int64  \n",
      " 5   NumberOfDependents               10000 non-null  int64  \n",
      " 6   NumberOfOpenCreditLinesAndLoans  10000 non-null  int64  \n",
      " 7   MonthlyIncome                    10000 non-null  int64  \n",
      " 8   NumberOfTimesPastDue             10000 non-null  int64  \n",
      " 9   EmploymentLength                 10000 non-null  int64  \n",
      " 10  HomeOwnership                    10000 non-null  object \n",
      " 11  Education                        10000 non-null  object \n",
      " 12  MaritalStatus                    10000 non-null  object \n",
      " 13  YearsAtCurrentAddress            10000 non-null  int64  \n",
      " 14  NoPaidPerc                       10000 non-null  float64\n",
      "dtypes: float64(2), int64(10), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "\n",
      "Datos nuevos:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1058 entries, 0 to 1057\n",
      "Data columns (total 15 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   ID                               1058 non-null   int64  \n",
      " 1   CreditScore                      1058 non-null   int64  \n",
      " 2   DebtRatio                        1058 non-null   float64\n",
      " 3   Assets                           1058 non-null   int64  \n",
      " 4   Age                              1058 non-null   int64  \n",
      " 5   NumberOfDependents               1058 non-null   int64  \n",
      " 6   NumberOfOpenCreditLinesAndLoans  1058 non-null   int64  \n",
      " 7   MonthlyIncome                    1058 non-null   int64  \n",
      " 8   NumberOfTimesPastDue             1058 non-null   int64  \n",
      " 9   EmploymentLength                 1058 non-null   int64  \n",
      " 10  HomeOwnership                    1058 non-null   object \n",
      " 11  Education                        1058 non-null   object \n",
      " 12  MaritalStatus                    1058 non-null   object \n",
      " 13  YearsAtCurrentAddress            1058 non-null   int64  \n",
      " 14  NewLoanApplication               1058 non-null   float64\n",
      "dtypes: float64(2), int64(10), object(3)\n",
      "memory usage: 124.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Importación de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Cargar datos históricos y nuevos\n",
    "tabla_nuevos = pd.read_csv('https://raw.githubusercontent.com/juancamiloespana/LEA3_FIN/main/data/datos_nuevos_creditos.csv')\n",
    "tabla_hist = pd.read_csv('https://raw.githubusercontent.com/juancamiloespana/LEA3_FIN/main/data/datos_historicos.csv')\n",
    "\n",
    "# Información inicial de los datos\n",
    "print(\"Datos históricos:\")\n",
    "print(tabla_hist.info())\n",
    "print(\"\\nDatos nuevos:\")\n",
    "print(tabla_nuevos.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X (variables explicativas): (10000, 10)\n",
      "Dimensiones de y (variable objetivo): (10000,)\n",
      "Columnas numéricas: Index(['CreditScore', 'DebtRatio', 'Assets', 'Age', 'NumberOfDependents',\n",
      "       'NumberOfOpenCreditLinesAndLoans', 'MonthlyIncome',\n",
      "       'NumberOfTimesPastDue', 'EmploymentLength', 'YearsAtCurrentAddress'],\n",
      "      dtype='object')\n",
      "Columnas categóricas: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos columnas no útiles\n",
    "tabla_hist.drop(columns=['ID', 'HomeOwnership', 'Education', 'MaritalStatus'], inplace=True)\n",
    "\n",
    "# Separación de variables explicativas (X) y objetivo (y)\n",
    "xtrain = tabla_hist.iloc[:, :-1]  # Todas las columnas excepto la última\n",
    "ytrain = tabla_hist[\"NoPaidPerc\"]  # Columna objetivo\n",
    "\n",
    "# Identificar columnas numéricas y categóricas\n",
    "colum_numericas = xtrain.select_dtypes(exclude='object').columns\n",
    "colum_categoricas = xtrain.select_dtypes(include='object').columns\n",
    "\n",
    "# Verificar dimensiones después de la separación\n",
    "print(\"Dimensiones de X (variables explicativas):\", xtrain.shape)\n",
    "print(\"Dimensiones de y (variable objetivo):\", ytrain.shape)\n",
    "print(\"Columnas numéricas:\", colum_numericas)\n",
    "print(\"Columnas categóricas:\", colum_categoricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X escalado: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Escalado de variables numéricas\n",
    "scaler = StandardScaler()\n",
    "xtrain_scaled = scaler.fit_transform(xtrain)\n",
    "\n",
    "# Confirmamos las dimensiones después del escalado\n",
    "print(\"Dimensiones de X escalado:\", xtrain_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntajes de validación cruzada (R2): [0.21303382 0.20633444 0.260819   0.23697766 0.14162505]\n",
      "Promedio R2: 0.21175799435309167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Modelo inicial\n",
    "rfr = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Validación cruzada\n",
    "scores = cross_val_score(rfr, xtrain_scaled, ytrain, cv=5, scoring='r2')\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Puntajes de validación cruzada (R2):\", scores)\n",
    "print(\"Promedio R2:\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables seleccionadas: Index(['DebtRatio'], dtype='object')\n",
      "Dimensiones de X reducido: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Modelos para la selección de variables\n",
    "modelos = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor(random_state=42), GradientBoostingRegressor(random_state=42)]\n",
    "\n",
    "# Definimos una función auxiliar para seleccionar las variables importantes )\n",
    "def sel_variables(modelos, x, y, threshold):\n",
    "    importancia = np.zeros(x.shape[1])\n",
    "    for modelo in modelos:\n",
    "        modelo.fit(x, y)\n",
    "        if hasattr(modelo, 'feature_importances_'):\n",
    "            importancia += modelo.feature_importances_\n",
    "        elif hasattr(modelo, 'coef_'):\n",
    "            importancia += np.abs(modelo.coef_)\n",
    "    importancia = importancia / len(modelos)  # Promedio de importancia\n",
    "    umbral = eval(threshold.replace(\"mean\", str(np.mean(importancia))))\n",
    "    indices_seleccionados = np.where(importancia >= umbral)[0]\n",
    "    return x.columns[indices_seleccionados]\n",
    "\n",
    "# Selección de variables importantes\n",
    "xtrain_df = pd.DataFrame(xtrain_scaled, columns=colum_numericas)  # Convertimos a DataFrame\n",
    "var_names = sel_variables(modelos, xtrain_df, ytrain, threshold=\"2.2*mean\")\n",
    "\n",
    "# Reducir el conjunto de entrenamiento a las variables seleccionadas\n",
    "xtrain_reduced = xtrain_df[var_names]\n",
    "\n",
    "# Mostrar las variables seleccionadas\n",
    "print(\"Variables seleccionadas:\", var_names)\n",
    "print(\"Dimensiones de X reducido:\", xtrain_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntajes de validación cruzada (R2) con variables seleccionadas: [-0.37156652 -0.43194695 -0.38365393 -0.3367181  -0.34037359]\n",
      "Promedio R2 con variables seleccionadas: -0.37285181766873726\n"
     ]
    }
   ],
   "source": [
    "# Validación cruzada con las variables seleccionadas\n",
    "scores_reduced = cross_val_score(rfr, xtrain_reduced, ytrain, cv=5, scoring='r2')\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Puntajes de validación cruzada (R2) con variables seleccionadas:\", scores_reduced)\n",
    "print(\"Promedio R2 con variables seleccionadas:\", np.mean(scores_reduced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevas variables seleccionadas: Index(['DebtRatio', 'Age', 'EmploymentLength'], dtype='object')\n",
      "Dimensiones de X reducido ajustado: (10000, 3)\n",
      "Puntajes de validación cruzada (R2) con variables ajustadas: [ 0.01394739  0.026485    0.04559744  0.02211818 -0.02914297]\n",
      "Promedio R2 con variables ajustadas: 0.015801006338135415\n"
     ]
    }
   ],
   "source": [
    "# Ajustar el umbral para incluir más variables\n",
    "var_names_adjusted = sel_variables(modelos, xtrain_df, ytrain, threshold=\"1.0*mean\")  # Reducimos el umbral\n",
    "xtrain_reduced_adjusted = xtrain_df[var_names_adjusted]\n",
    "\n",
    "# Validación cruzada con el nuevo conjunto reducido\n",
    "scores_adjusted = cross_val_score(rfr, xtrain_reduced_adjusted, ytrain, cv=5, scoring='r2')\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Nuevas variables seleccionadas:\", var_names_adjusted)\n",
    "print(\"Dimensiones de X reducido ajustado:\", xtrain_reduced_adjusted.shape)\n",
    "print(\"Puntajes de validación cruzada (R2) con variables ajustadas:\", scores_adjusted)\n",
    "print(\"Promedio R2 con variables ajustadas:\", np.mean(scores_adjusted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntajes de validación cruzada (R2) con todas las variables: [0.21303382 0.20633444 0.260819   0.23697766 0.14162505]\n",
      "Promedio R2 con todas las variables: 0.21175799435309167\n"
     ]
    }
   ],
   "source": [
    "# Validación cruzada con todas las variables\n",
    "scores_all = cross_val_score(rfr, xtrain_scaled, ytrain, cv=5, scoring='r2')\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Puntajes de validación cruzada (R2) con todas las variables:\", scores_all)\n",
    "print(\"Promedio R2 con todas las variables:\", np.mean(scores_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.182 total time=   0.8s\n",
      "[CV 2/3] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.218 total time=   0.8s\n",
      "[CV 3/3] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.136 total time=   0.8s\n",
      "[CV 2/3] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.218 total time=   0.8s\n",
      "[CV 1/3] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.182 total time=   0.7s\n",
      "[CV 3/3] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.136 total time=   0.8s\n",
      "[CV 2/3] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.221 total time=   1.5s\n",
      "[CV 1/3] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.183 total time=   1.6s\n",
      "[CV 1/3] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.183 total time=   0.8s\n",
      "[CV 3/3] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.137 total time=   1.7s\n",
      "[CV 2/3] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.218 total time=   0.8s\n",
      "[CV 2/3] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.221 total time=   1.6s\n",
      "[CV 1/3] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.183 total time=   1.6s\n",
      "[CV 3/3] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.136 total time=   0.8s\n",
      "[CV 3/3] END max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.137 total time=   1.6s\n",
      "[CV 1/3] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.183 total time=   0.8s\n",
      "[CV 2/3] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.218 total time=   0.8s\n",
      "[CV 3/3] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.136 total time=   0.8s\n",
      "[CV 1/3] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.184 total time=   1.6s\n",
      "[CV 2/3] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.221 total time=   1.6s\n",
      "[CV 3/3] END max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.138 total time=   1.6s\n",
      "[CV 1/3] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.184 total time=   1.5s\n",
      "[CV 2/3] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.221 total time=   1.6s\n",
      "[CV 1/3] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.190 total time=   1.5s\n",
      "[CV 3/3] END max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.138 total time=   1.5s\n",
      "[CV 2/3] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.246 total time=   1.5s\n",
      "[CV 3/3] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.162 total time=   1.5s\n",
      "[CV 1/3] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.193 total time=   1.5s\n",
      "[CV 2/3] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.250 total time=   1.5s\n",
      "[CV 3/3] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.160 total time=   1.5s\n",
      "[CV 1/3] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.198 total time=   2.9s\n",
      "[CV 2/3] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.250 total time=   3.1s\n",
      "[CV 3/3] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.163 total time=   3.0s\n",
      "[CV 2/3] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.253 total time=   1.4s\n",
      "[CV 1/3] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.198 total time=   3.0s\n",
      "[CV 1/3] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.196 total time=   1.5s\n",
      "[CV 3/3] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.158 total time=   1.5s\n",
      "[CV 2/3] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.252 total time=   3.0s\n",
      "[CV 3/3] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100;, score=0.163 total time=   2.9s\n",
      "[CV 2/3] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.255 total time=   1.5s\n",
      "[CV 3/3] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.159 total time=   1.4s\n",
      "[CV 1/3] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50;, score=0.197 total time=   1.5s\n",
      "[CV 1/3] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.204 total time=   2.8s\n",
      "[CV 2/3] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.257 total time=   2.8s\n",
      "[CV 3/3] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.161 total time=   2.6s\n",
      "[CV 1/3] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.204 total time=   2.5s\n",
      "[CV 2/3] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.258 total time=   2.3s\n",
      "[CV 3/3] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100;, score=0.162 total time=   2.1s\n",
      "Mejores parámetros encontrados: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Espacio de búsqueda reducido\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],       # Reducimos el número de árboles\n",
    "    'max_depth': [5, 10],            # Reducimos las profundidades\n",
    "    'min_samples_split': [2, 5],     # Menos opciones para división mínima\n",
    "    'min_samples_leaf': [1, 2]       # Menos opciones para hojas mínimas\n",
    "}\n",
    "\n",
    "# Configuramos GridSearchCV\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='r2',\n",
    "                           cv=3,          # Reducimos el número de folds\n",
    "                           n_jobs=-1,     # Paralelización\n",
    "                           verbose=3)     # Mostrar progreso\n",
    "\n",
    "# Ajustamos el modelo\n",
    "grid_search.fit(xtrain_scaled, ytrain)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntajes de validación cruzada (R2) con el modelo optimizado: [0.22001679 0.2306006  0.26748879 0.24495169 0.14458561]\n",
      "Promedio R2 con el modelo optimizado: 0.22152869561174\n"
     ]
    }
   ],
   "source": [
    "# Construimos el modelo final con los mejores parámetros\n",
    "best_rfr = RandomForestRegressor(max_depth=10, \n",
    "                                 min_samples_leaf=2, \n",
    "                                 min_samples_split=5, \n",
    "                                 n_estimators=100, \n",
    "                                 random_state=42)\n",
    "\n",
    "# Validación cruzada con el modelo optimizado\n",
    "scores_best = cross_val_score(best_rfr, xtrain_scaled, ytrain, cv=5, scoring='r2')\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(\"Puntajes de validación cruzada (R2) con el modelo optimizado:\", scores_best)\n",
    "print(\"Promedio R2 con el modelo optimizado:\", np.mean(scores_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones guardadas en 'grupo_7.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Alinear las columnas de los datos nuevos con las de los datos históricos\n",
    "x_nuevos = tabla_nuevos.drop(columns=['ID'])  # Eliminamos la columna ID\n",
    "\n",
    "# Codificar las columnas categóricas usando los valores únicos de los datos históricos\n",
    "categorical_cols = ['HomeOwnership', 'Education', 'MaritalStatus']\n",
    "x_nuevos = pd.get_dummies(x_nuevos, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Alinear las columnas de los datos nuevos con las del modelo (rellenamos las faltantes con ceros)\n",
    "x_nuevos = x_nuevos.reindex(columns=xtrain_df.columns, fill_value=0)\n",
    "\n",
    "# Escalar los datos nuevos con el mismo escalador\n",
    "x_nuevos_scaled = scaler.transform(x_nuevos)\n",
    "\n",
    "# Generar predicciones con el modelo optimizado\n",
    "best_rfr.fit(xtrain_scaled, ytrain)  # Entrenamos el modelo con todos los datos históricos\n",
    "predicciones = best_rfr.predict(x_nuevos_scaled)\n",
    "\n",
    "# Sumar 15 al interés base (según requerimiento)\n",
    "predicciones_ajustadas = predicciones \n",
    "\n",
    "# Crear un DataFrame con los resultados y columnas personalizadas\n",
    "resultados = pd.DataFrame({\n",
    "    \"ID\": tabla_nuevos[\"ID\"],\n",
    "    \"int_rc\": predicciones_ajustadas\n",
    "})\n",
    "\n",
    "# Guardar los resultados en un archivo CSV delimitado por comas\n",
    "resultados.to_csv(\"nuevogrupo_7.csv\", index=False)\n",
    "print(\"Predicciones guardadas en 'grupo_7.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
